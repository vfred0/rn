{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "THbqIs45LGkg"
   },
   "source": [
    "# Visualización del proceso de aprendizaje en Regresión Logística 1D con descenso de gradiente\n",
    "\n",
    "Nota: La visualización solo funciona en un entorno Jupyter local (no en google colab)\n",
    "\n",
    "Funciones auxiliares (solo ejecutar, y seguir más abajo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8726,
     "status": "ok",
     "timestamp": 1648500894275,
     "user": {
      "displayName": "Redes Neuronales",
      "userId": "11347185938143135425"
     },
     "user_tz": 180
    },
    "id": "kxVVCknaLGkh",
    "outputId": "6eecc154-3536-43d2-fe98-f45a38d8c9a5"
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "%matplotlib notebook\n",
    "\n",
    "!pip install -q rnutil\n",
    "import rnutil\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.patches as patches\n",
    "from collections import namedtuple\n",
    "from matplotlib import cm\n",
    "\n",
    "PlotState = namedtuple('PlotState', ['figure','axes','legends'])\n",
    "OptimalModel = namedtuple('Model', ['m', 'b','error'])\n",
    "\n",
    "def optimal_model(x,y):\n",
    "    from sklearn import datasets, linear_model\n",
    "    regr = linear_model.LogisticRegression(C=1)\n",
    "    regr.fit(x.reshape(-1, 1),y)\n",
    "    optimal_m=regr.coef_[0][0]\n",
    "    optimal_b=regr.intercept_[0]\n",
    "    optimal_yhat = forward(x,optimal_m,optimal_b)\n",
    "    optimal_error=mean_binary_cross_entropy(y,optimal_yhat)\n",
    "    return OptimalModel(optimal_m,optimal_b,optimal_error)\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "def linear(x,m,b):\n",
    "    return x*m+b\n",
    "\n",
    "def forward(x,m,b):\n",
    "    return sigmoid(linear(x,m,b))\n",
    "\n",
    "def mean_binary_cross_entropy(y,yhat):\n",
    "    eps = np.finfo(float).eps\n",
    "    n=len(y)\n",
    "    errors=np.zeros(n)\n",
    "    for i in range(n):\n",
    "        if y[i]==1:\n",
    "            errors[i]=-np.log(yhat[i]+eps)# agregamos un valor chico por si yhat[i]=0\n",
    "        else: #y[i]==0\n",
    "            errors[i]=-np.log(1-yhat[i]+eps)\n",
    "            \n",
    "    # implementación vectorial\n",
    "    #errors=y* (-log(yhat)) +(1-y)* (-log(-yhat))\n",
    "    return errors.mean()\n",
    "    \n",
    "\n",
    "def backward(x,y,m,b):\n",
    "    dEdm=0\n",
    "    dEdb=0\n",
    "    n=len(x)\n",
    "    fx=forward(x,m,b)\n",
    "    for i in range(n):\n",
    "        dEdmi=(fx[i]-y[i])*x[i]\n",
    "        dEdm+=dEdmi\n",
    "        dEdbi=(fx[i]-y[i])\n",
    "        dEdb+=dEdbi\n",
    "    return dEdm,dEdb\n",
    "\n",
    "def visualizar_modelo(ax_data,x,m,b):\n",
    "    x_pad=70\n",
    "    min_x,max_x=x.min()-x_pad,x.max()+x_pad\n",
    "    x_plot=np.linspace(min_x,max_x,40)\n",
    "    y_plot=forward(x_plot,m,b)\n",
    "    ax_data.plot(x_plot,y_plot,'-',color=\"red\")\n",
    "    \n",
    "def init_data(ax_data,x,y,optimal,m,b):\n",
    "    ax_data.scatter(x,y,color=\"blue\")\n",
    "    \n",
    "    visualizar_modelo(ax_data,x,optimal.m,optimal.b)\n",
    "    \n",
    "    ax_data.set_xlabel(\"x (Horas estudiadas)\")\n",
    "    ax_data.set_ylabel(\"y p(aprobar | x)\")\n",
    "    \n",
    "def init_error_history(ax_error):\n",
    "    ax_error.set_xlabel(\"Iteración\")\n",
    "    ax_error.set_ylabel(\"Error\")\n",
    "    ax_error.set_title(\"Historial de Error\")\n",
    "    \n",
    "def init_contour(ax_contour,x,y,optimal,m,b):    \n",
    "    ax_contour.set_xlabel(\"m\")\n",
    "    ax_contour.set_ylabel(\"b\")\n",
    "    ax_contour.set_title(\"Contorno de E(m,b) \")\n",
    "    \n",
    "    samples=100\n",
    "    param_range=6\n",
    "    M = np.linspace(-param_range, param_range, samples)\n",
    "    B = np.linspace(-param_range, param_range, samples)    \n",
    "    ms, bs = np.meshgrid(M, B)\n",
    "    \n",
    "    es=np.zeros_like(ms)\n",
    "    n_m=ms.shape[0]\n",
    "    n_b=ms.shape[0]\n",
    "    \n",
    "    for i in range(n_m):\n",
    "        for j in range(n_b):\n",
    "            y_hat = forward(x,ms[i,j],bs[i,j])\n",
    "            es[i,j]=mean_binary_cross_entropy(y,y_hat)\n",
    "\n",
    "    surf=ax_contour.contourf(ms,bs,es,cmap='coolwarm')\n",
    "    \n",
    "    ax_contour.scatter([optimal.m], [optimal.b],c=\"green\")\n",
    "    plt.colorbar(surf, shrink=0.5, aspect=5)\n",
    "    \n",
    "def init_plot(x,y,optimal,m,b):\n",
    "    \n",
    "    figure=plt.figure(dpi=100,figsize=(14,6))\n",
    "    plt.suptitle('Entrenamiento de modelo logístico')\n",
    "    ax_data=figure.add_subplot(1,3,1)\n",
    "    ax_error=figure.add_subplot(1,3,2)\n",
    "    ax_contour=figure.add_subplot(1,3,3)#,projection='3d')\n",
    "    axes=(ax_data,ax_error,ax_contour)\n",
    "    \n",
    "    init_data(ax_data,x,y,optimal,m,b)\n",
    "    init_error_history(ax_error)\n",
    "    init_contour(ax_contour,x,y,optimal,m,b)\n",
    "\n",
    "    return PlotState(figure,axes,None)\n",
    "\n",
    "def visualizar(plot_state,x,y,iteration,m,b,dEdm,dEdb,optimal,max_iterations,error_history):\n",
    "    (ax1,ax2,ax3)=plot_state.axes\n",
    "    # Visualizacion\n",
    "    \n",
    "    #actualizar linea del modelo actual\n",
    "    ax1.lines.pop()\n",
    "    visualizar_modelo(ax1,x,m,b)\n",
    "#     ax3.scatter(iteration_state.m, iteration_state.b, iteration_state.mean_error,c=\"black\",s=50)\n",
    "    ax3.scatter([m], [b],c=\"black\",s=5)\n",
    "    \n",
    "    # Mostrar leyendas\n",
    "    model = patches.Patch(color='red', label='Modelo: y=$\\\\sigma$(x*({:.2f})+({:.2f}))'.format(m,b))\n",
    "    label='Modelo de sklearn: y=$\\\\sigma$(x*({:.2f})+({:.2f}))'.format(optimal.m,optimal.b)\n",
    "    model_true = patches.Patch(color='cyan', label=label)\n",
    "    label='$\\\\frac{ \\\\partial E}{\\\\partial m}=$ %.2f, $\\\\frac{ \\\\partial E}{\\\\partial b}$ %.2f' %(dEdm,dEdb)\n",
    "    derivatives = patches.Patch(color='red', label=label)\n",
    "    error_patch = patches.Patch(color='red', label='$E=\\\\frac{1}{n} \\sum_i^n E_i=$ %.2f' % (error_history[-1]))\n",
    "    optimal_error = patches.Patch(color='cyan', label='E del modelo de sklearn: %.2f' % (optimal.error))\n",
    "    handles=[model,model_true,derivatives,error_patch,optimal_error]\n",
    "    \n",
    "    ax2.lines.clear()\n",
    "    ax2.plot(error_history)\n",
    "    ax2.set_xlim(0,max_iterations)\n",
    "    miny,maxy=ax2.get_ylim()\n",
    "    ax2.set_ylim(0,maxy)\n",
    "    ax2.set_title(\"Curva de error de entrenamiento\")\n",
    "    ax2.legend(handles=handles)\n",
    "    \n",
    "    ax1.set_title(\"Iteración %03d / %03d\" % (iteration+1,max_iterations))\n",
    "    \n",
    "    plot_state.figure.canvas.draw()\n",
    "        \n",
    "    \n",
    "\n",
    "###############################################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TzYXq6AgLGkj"
   },
   "source": [
    "# Visualización del modelo y la superficie del error durante el aprendizaje\n",
    "\n",
    "El siguiente código entrena un modelo de regresión logistica 1D `f(x)=sigmoid(mx+b)` con descenso de gradiente.\n",
    "\n",
    "Para eso se muestran 3 gráficos durante la optimización:\n",
    "* Los datos y el modelo, con los valores del error medio y las derivadas de los parámetros.\n",
    "* La curva de error, que indica el error del modelo para cada iteración del algoritmo\n",
    "* La superficie del error, que en este caso se muestra como el _contorno_ del error, es decir, como la superficie del error vista de arriba, y utilizando colores para indicar los valores del error.\n",
    "\n",
    "Estos gráficos se actualizan en cada iteración del algoritmo, de modo de poder ver el recorrido de los valores `m` y `b`, como eso afecta al modelo, y como va minimizándose el error en consecuencia.\n",
    "\n",
    "Probá modificando:\n",
    "* Los valores iniciales de los parámetros `m` y `b`\n",
    "* La tasa de aprendizaje `α`\n",
    "* La cantidad de iteraciones máximas `max_iterations`\n",
    "\n",
    "Y comprendiendo como estos valores inciden en el algoritmo. Respondé:\n",
    "\n",
    "* ¿Qué valores de `α` permiten que el algoritmo converja? ¿y cuales que converja en un tiempo razonable?\n",
    "* ¿Se alcanza siempre el mínimo global (punto verde)?\n",
    "* ¿Qué parámetro importa más para la optimización? La magnitud de las derivadas ¿es la misma para ambos?\n",
    "* ¿Se avanza lo mismo en todas las iteraciones (relacionar con magnitud de derivadas)?\n",
    "\n",
    "\n",
    "Luego de eso, entre el comentario `## COMIENZO NORMALIZAR` y `##FIN NORMALIZAR`, normalizá los datos de entrada `x` restándoles la media μ y dividiendo por la desviación estándar σ con la fórmula `x ← (x-μ)/σ`. Recordá que `x` es un vector de NumPy y por ende soporta los métodos `mean()` y `std()`. \n",
    "\n",
    "Volvé a responder las preguntas anteriores ahora con los datos normalizados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 598
    },
    "executionInfo": {
     "elapsed": 12693,
     "status": "ok",
     "timestamp": 1648500937006,
     "user": {
      "displayName": "Redes Neuronales",
      "userId": "11347185938143135425"
     },
     "user_tz": 180
    },
    "id": "UYiRutIELGkk",
    "outputId": "3430d25f-4e2b-4871-b43e-cddbc7bacb75",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Carga de datos\n",
    "data = rnutil.load_dataset_numpy(\"study_logistic.csv\")\n",
    "x,y=data[:,0],data[:,1]\n",
    "\n",
    "\n",
    "## Parámetros iniciales del modelo (probar valores entre -5 y 5)\n",
    "m=0\n",
    "b=0\n",
    "# Configuración del descenso de gradiente\n",
    "\n",
    "#iteraciones máximas a realizar\n",
    "max_iterations=3000\n",
    "#velocidad de aprendizaje\n",
    "#valores sensatos: entre 0.00001 y 0.001\n",
    "alpha=0.0002\n",
    "#fin Configuración\n",
    "\n",
    "# Cálculo del valor óptimo por sklearn\n",
    "optimal=optimal_model(x,y)\n",
    "# fin cálculo del valor óptimo por sklearn\n",
    "\n",
    "# Inicialización del algoritmo\n",
    "iteration=0\n",
    "mean_error=0\n",
    "error_history=[]\n",
    "# Fin inicialización del algoritmo\n",
    "\n",
    "# Opciones de visualización \n",
    "\n",
    "fps=30\n",
    "#Fin opciones de visualización\n",
    "plot_state=init_plot(x,y,optimal,m,b)\n",
    "error_history=[]\n",
    "\n",
    "while iteration<max_iterations:\n",
    "    \n",
    "    #DESCENSO DE GRADIENTE\n",
    "\n",
    "    #calculo de derivadas\n",
    "    dEdm,dEdb=backward(x,y,m,b)\n",
    "   \n",
    "    #CALCULO DE ERROR\n",
    "    #Calculo del error del modelo\n",
    "    yhat=forward(x,m,b)\n",
    "    error = mean_binary_cross_entropy(y,yhat)\n",
    "    error_history.append(error)\n",
    "    #FIN CALCULO DE ERROR\n",
    "    \n",
    "    #visualización\n",
    "    if (iteration % 100 ==0):\n",
    "        visualizar(plot_state,x,y,iteration,m,b,dEdm,dEdb,optimal,max_iterations,error_history)\n",
    "        plt.pause(1/fps)\n",
    "    # fin visualización\n",
    "    \n",
    "    #actualizo los parámetros\n",
    "    m=m-alpha*dEdm\n",
    "    b=b-alpha*dEdb\n",
    "    # FIN DESCENSO DE GRADIENTE\n",
    "    \n",
    "    iteration+=1"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Regresion Logística 1D - Aprendizaje.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
