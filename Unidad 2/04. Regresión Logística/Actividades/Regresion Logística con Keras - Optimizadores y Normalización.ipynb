{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "KGzWert7cNuF"
   },
   "source": [
    "Regresion Logística con Keras\n",
    "\n",
    "En este ejercicio, tu objetivo será entrenar modelos de Regresión Logística utilizando Keras (y Tensorflow como backend) para familiarizarte con la librería.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9PTXUGA7cNuG"
   },
   "outputs": [],
   "source": [
    "\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "!pip install -q rnutil\n",
    "import rnutil\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9-XeSzlkcNuQ"
   },
   "source": [
    "# Entrena un modelo de Regresión Logística con Keras para el dataset de estudio 2D\n",
    "\n",
    "El siguiente código carga un dataset de prueba con 2 dimensiones de entrada y una de salida.\n",
    "\n",
    "Luego crea un modelo de regresión logística con Keras, y visualiza sus pesos iniciales. \n",
    "\n",
    "Es importante notar tres cosas:\n",
    "\n",
    "1. La métrica utilizada es `'sparse_categorical_crossentropy'`, es decir la entropía cruzada. Esta es la misma métrica vista en la teoría de Regresión Logística Múltiple.\n",
    "\n",
    "2. El optimizador es una clase que define el algoritmo para minimizar el error cuadrático. En general, son todas variantes de descenso de gradiente. En este caso, estamos utilizando descenso de gradiente estocástico (`keras.optimizers.SGD`), que es igual al descenso de gradiente pero realiza cada actualización de los parámetros con un subconjunto de los ejemplos del dataset. \n",
    "\n",
    "3. El método para entrenar el modelo es `fit`. En este caso, el parámetro `lr` lo recibe el optimizador, pero `fit` recibe la cantidad de iteraciones (`epochs`) y el tamaño del batch para el SGD (`batch_size`).\n",
    "\n",
    "\n",
    "Al finalizar el entrenamiento, observá los valores del vector de pesos `w`. ¿A qué atributo o variable de entrada le da más importancia el modelo?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U3UTcRC3cNuU"
   },
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import utils \n",
    "\n",
    "# Carga del dataset\n",
    "\n",
    "data=rnutil.load_dataset_numpy(\"study_logistic_2d.csv\")\n",
    "x,y=data[:,0:2],data[:,2]\n",
    "# cantidad de ejemplos y dimension de entrada\n",
    "n,d_in=x.shape\n",
    "\n",
    "\n",
    "# calcula la cantidad de clases\n",
    "classes=int(y.max()+1)\n",
    "\n",
    "print(\"Información del conjunto de datos:\")\n",
    "print(f\"Ejemplos: {n}\")\n",
    "print(f\"Variables de entrada: {d_in}\")\n",
    "print(f\"Cantidad de clases: {classes}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Creación del modelo logítico\n",
    "print(\"Inicialización aleatoria del modelo (podes volver a correr esta celda para obtener otros resultados)\")\n",
    "# Creo un modelo lineal\n",
    "modelo = keras.Sequential([\n",
    "    # la activación softmax hace que la salida sean probabilidades\n",
    "    keras.layers.Dense(classes,input_shape=(d_in,), activation='softmax')])\n",
    "\n",
    "# visualización del modelo inicial\n",
    "rnutil.plot_fronteras_keras(modelo,x,y,title=\"Modelo inicial\")\n",
    "\n",
    "\n",
    "modelo.compile(\n",
    "  optimizer=keras.optimizers.SGD(learning_rate=0.001), \n",
    "  loss='sparse_categorical_crossentropy', \n",
    "  # metricas para ir calculando en cada iteracion \n",
    "  # Agregamos el accuracy del modelo\n",
    "  metrics=['accuracy'], \n",
    ")\n",
    "\n",
    "# Entrenamiento del modelo\n",
    "history = modelo.fit(x,y,epochs=200,batch_size=8,verbose=False)\n",
    "rnutil.plot_loss_accuracy_keras(history)\n",
    "\n",
    "# visualiza el modelo y los datos\n",
    "w,b=modelo.get_weights()\n",
    "print(f\"w: {w}, b: {b}\")\n",
    "rnutil.plot_fronteras_keras(modelo,x,y,title=\"Modelo Final\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "awgeC0RucNub"
   },
   "source": [
    "# Reentrenamiento con otro optimizador\n",
    "\n",
    "En la mayoría de los casos, el modelo no converge adecuadamente, aún cambiando la tasa de aprendizaje. Podemos utilizar el optimizador [Adam](https://arxiv.org/abs/1412.6980), un optimizador avanzado que muchas veces logra que el modelo converja aún cuando `SGD` no puede.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O4ueodPAcNue"
   },
   "outputs": [],
   "source": [
    "modelo = keras.Sequential([\n",
    "    # la activación softmax hace que la salida sean probabilidades\n",
    "    keras.layers.Dense(classes,input_shape=(d_in,), activation='softmax')])\n",
    "\n",
    "modelo.compile(\n",
    "  optimizer=keras.optimizers.Adam(learning_rate=0.001), # Cambiamos el optimizador a ADAM\n",
    "  loss='sparse_categorical_crossentropy', \n",
    "  metrics=['accuracy'], \n",
    ")\n",
    "\n",
    "# Entrenamiento del modelo\n",
    "history = modelo.fit(x,y,epochs=1000,batch_size=8,verbose=False)\n",
    "\n",
    "rnutil.plot_loss_accuracy_keras(history)\n",
    "rnutil.plot_fronteras_keras(modelo,x,y,title=\"Modelo Final con Adam\")\n",
    "w,b=modelo.get_weights()\n",
    "print(f\"w: {w}, b: {b}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dH8ikymBcNul"
   },
   "source": [
    "# Entrenamiento con normalización de los datos\n",
    "\n",
    "Si bien un mejor optimizador puede ayudarnos a mejorar el modelo final a obtener, otra opción maś simple (aunque no excluyente) y a veces más efectiva es normalizar los datos de entrada. En este caso vemos como dicha normalización permite que el modelo siempre converja. \n",
    "\n",
    "**Implementa la normalización media/desviación** estándar de los datos de la variable `x_norm`, que es una copia de `x`.\n",
    "\n",
    "Al finalizar el entrenamiento, observá los valores del vector de pesos `w`. ¿A qué atributo o variable de entrada le da más importancia el modelo? ¿Cambió con la normalización? ¿Cómo fue el proceso de optimización respecto de los anteriores?\n",
    "\n",
    "** IMPORTANTE ** Si no implementás la normalización no vas a ver los cambios en el entrenamiento y valores finales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q9LtkflscNul"
   },
   "outputs": [],
   "source": [
    "x_norm=x.copy()\n",
    "# Normalizacion mu/std de los datos de entrada\n",
    "# TODO\n",
    "for i in range(d_in):\n",
    "    # normalizo la columna i restando su media y dividiendo por su desv. est.\n",
    "    pass    \n",
    "# fin TODO\n",
    "\n",
    "\n",
    "    \n",
    "modelo = keras.Sequential([\n",
    "    # la activación softmax hace que la salida sean probabilidades\n",
    "    keras.layers.Dense(classes,input_shape=(d_in,), activation='softmax')])\n",
    "\n",
    "\n",
    "\n",
    "rnutil.plot_fronteras_keras(modelo,x_norm,y,title=\"Modelo Inicial\")\n",
    "\n",
    "modelo.compile(\n",
    "  optimizer=keras.optimizers.SGD(learning_rate=0.001), \n",
    "  loss='sparse_categorical_crossentropy', \n",
    "  metrics=['accuracy'], \n",
    ")\n",
    "\n",
    "# Entrenamiento del modelo\n",
    "history = modelo.fit(x_norm,y,epochs=1000,batch_size=8,verbose=False)\n",
    "rnutil.plot_loss_accuracy_keras(history)\n",
    "\n",
    "# visualiza el modelo y los datos\n",
    "\n",
    "rnutil.plot_fronteras_keras(modelo,x_norm,y,title=\"Modelo Final\")\n",
    "w,b=modelo.get_weights()\n",
    "print(f\"w: {w}, b: {b}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Regresion Logística con Keras - Clase .ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
