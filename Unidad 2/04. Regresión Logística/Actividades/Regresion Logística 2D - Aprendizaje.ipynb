{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualización del proceso de aprendizaje en Regresión Logística 2D con descenso de gradiente\n",
    "\n",
    "Funciones auxiliares (solo ejecutar, y seguir más abajo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "%matplotlib notebook\n",
    "!pip install -q rnutil\n",
    "import rnutil\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.patches as patches\n",
    "from collections import namedtuple\n",
    "from matplotlib import cm\n",
    "\n",
    "PlotState = namedtuple('PlotState', ['figure','axes','legends'])\n",
    "OptimalModel = namedtuple('Model', ['w', 'b','error'])\n",
    "\n",
    "def optimal_model(x,y):\n",
    "    from sklearn import datasets, linear_model\n",
    "    regr = linear_model.LogisticRegression(C=1)\n",
    "    regr.fit(x,y)\n",
    "    optimal_w=regr.coef_[0]\n",
    "    optimal_b=regr.intercept_[0]\n",
    "    optimal_error=evaluate_model(x,y,optimal_w,optimal_b)\n",
    "    return OptimalModel(optimal_w,optimal_b,optimal_error)\n",
    "\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "def linear(x,w,b):\n",
    "    return x.dot(w)+b\n",
    "\n",
    "def predict(x,w,b):\n",
    "    return sigmoid(linear(x,w,b))\n",
    "\n",
    "def binary_cross_entropy(y,yhat):\n",
    "    eps = np.finfo(float).eps\n",
    "    n=len(y)\n",
    "    errors=np.zeros(n)\n",
    "    for i in range(n):\n",
    "        if y[i]==1:\n",
    "            errors[i]=-np.log(yhat[i]+eps)\n",
    "        else: #y[i]==0\n",
    "            errors[i]=-np.log(1-yhat[i]+eps)\n",
    "            \n",
    "    # implementación vectorial\n",
    "    #errors=y* (-log(yhat)) +(1-y)* (-log(-yhat))\n",
    "    return errors.sum()\n",
    "    \n",
    "\n",
    "def gradient(x,y,w,b):\n",
    "    dEdw=np.zeros_like(w)\n",
    "    dEdb=0\n",
    "    n=len(x)\n",
    "    fx=predict(x,w,b)\n",
    "    for i in range(n):\n",
    "        dEdwi=(fx[i]-y[i])*x[i,:]\n",
    "        dEdw+=dEdwi\n",
    "        dEdbi=(fx[i]-y[i])\n",
    "        dEdb+=dEdbi\n",
    "    dEdwi=dEdwi/n\n",
    "    dEdb=dEdb/n\n",
    "    return dEdw,dEdb\n",
    "\n",
    "def visualizar_modelo(ax_data,x,w,b):\n",
    "    samples=100\n",
    "    xmin,xmax=x.min(),x.max()\n",
    "    param_range=max(abs(xmin),abs(xmax))+5\n",
    "    X = np.linspace(-param_range, param_range, samples)\n",
    "    Y = np.linspace(-param_range, param_range, samples)    \n",
    "    xx, yy = np.meshgrid(X, Y)\n",
    "    pp=np.zeros_like(xx)\n",
    "    ss=np.zeros_like(xx)\n",
    "    n,m=len(X),len(Y)\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            plot_x=np.array([xx[i,j],yy[i,j]])\n",
    "            pp[i,j]=predict(plot_x,w,b)\n",
    "            \n",
    "    surf=ax_data.contourf(xx,yy,pp,cmap='coolwarm',alpha=0.9)\n",
    "    return surf\n",
    "    \n",
    "def visualizar_datos(ax_data,x,y): \n",
    "    ax_data.scatter(x[y==1,0],x[y==1,1],color=\"red\")\n",
    "    ax_data.scatter(x[y==0,0],x[y==0,1],color=\"blue\")\n",
    "    \n",
    "    ax_data.set_xlabel(\"x (Horas estudiadas)\")\n",
    "    ax_data.set_ylabel(\"y (Promedio)\")\n",
    "    ax_data.set_title(\"P(aprobar) dadas las horas estudiadas y el promedio\")    \n",
    "    \n",
    "    \n",
    "def init_error_history(ax_error):\n",
    "    ax_error.set_xlabel(\"Iteración\")\n",
    "    ax_error.set_ylabel(\"Error\")\n",
    "    ax_error.set_title(\"Historial de Error\")\n",
    "    \n",
    "def init_plot(x,y,w,b):\n",
    "    \n",
    "    figure=plt.figure(dpi=100,figsize=(14,6))\n",
    "    plt.suptitle('Entrenamiento de modelo logístico')\n",
    "    ax_data=figure.add_subplot(1,2,1)\n",
    "    ax_error=figure.add_subplot(1,2,2)\n",
    "    axes=(ax_data,ax_error)\n",
    "    \n",
    "    visualizar_datos(ax_data,x,y)\n",
    "    surf=visualizar_modelo(ax_data,x,w,b)\n",
    "    plt.colorbar(surf, shrink=0.8, aspect=5,ax=ax_data)\n",
    "    init_error_history(ax_error)\n",
    "    \n",
    "\n",
    "    return PlotState(figure,axes,None)\n",
    "\n",
    "def visualizar(plot_state,x,y,iteration,w,b,dEdw,dEdb,max_iterations,error_history):\n",
    "    (ax_data,ax2)=plot_state.axes\n",
    "    # Visualizacion\n",
    "    \n",
    "    #actualizar linea del modelo actual\n",
    "    ax_data.clear()\n",
    "    visualizar_modelo(ax_data,x,w,b)\n",
    "    visualizar_datos(ax_data,x,y)\n",
    "\n",
    "    # Mostrar leyendas\n",
    "    model = patches.Patch(color='red', label='Modelo: w=%s, b=%.2f' % (str(w),b))\n",
    "\n",
    "    label='$\\\\frac{ \\\\partial E}{\\\\partial w}=$ %s, $\\\\frac{ \\\\partial E}{\\\\partial b}$ %.2f' %(str(dEdw),dEdb)\n",
    "    derivatives = patches.Patch(color='red', label=label)\n",
    "    label='$E = $  %.2f' %  (error_history[-1])\n",
    "    error_patch = patches.Patch(color='red', label=label)\n",
    "    handles=[model,derivatives,error_patch]\n",
    "    ax_data.legend(handles=handles)\n",
    "    \n",
    "    ax2.lines.clear()\n",
    "    ax2.plot(error_history,color=\"blue\")\n",
    "    miny,maxy=ax2.get_ylim()\n",
    "    ax2.set_ylim(0,maxy)\n",
    "    ax2.set_xlim(0,max_iterations)\n",
    "\n",
    "    ax_data.set_title(\"Iteración %03d / %03d\" % (iteration+1,max_iterations))\n",
    "    \n",
    "    plot_state.figure.canvas.draw()\n",
    "###############################################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualización del modelo y la curva de error durante el aprendizaje\n",
    "\n",
    "El siguiente código entrena un modelo de regresión logística `f(x)=sigmoid(w*x+b)` con descenso de gradiente.\n",
    "\n",
    "Para eso se muestran 2 gráficos durante la optimización:\n",
    "* Los datos y el modelo, con los valores del error medio y las derivadas de los parámetros.\n",
    "* La curva de error, que indica el error del modelo para cada iteración del algoritmo\n",
    "\n",
    "Estos gráficos se actualizan en cada iteración del algoritmo, de modo de poder ver el recorrido de los valores `w` y `b`, como eso afecta al modelo, y como va minimizándose el error en consecuencia.\n",
    "\n",
    "Probá modificando:\n",
    "* Los valores iniciales de los parámetros `w` y `b`\n",
    "* La tasa de aprendizaje `α`\n",
    "* La cantidad de iteraciones máximas `max_iterations`\n",
    "\n",
    "Y comprendiendo como estos valores inciden en el algoritmo. Respondé:\n",
    "\n",
    "* ¿Qué valores de `α` permiten que el algoritmo converja? ¿y cuales que converja en un tiempo razonable?\n",
    "* ¿Se alcanza siempre el mínimo global (plano verde)?\n",
    "* ¿Qué parámetro importa más para la optimización? La magnitud de las derivadas ¿es la misma para ambos?\n",
    "* ¿Se avanza lo mismo en todas las iteraciones (relacionar con magnitud de derivadas)?\n",
    "* ¿Cuál es el máximo valor de `α` antes de que el algoritmo comience a diverger?\n",
    "\n",
    "Luego de eso, entre el comentario `## COMIENZO NORMALIZAR` y `##FIN NORMALIZAR`, normalizá los datos de entrada `x` restándoles la media μ y dividiendo por la desviación estándar σ con la fórmula `x ← (x-μ)/σ`. Recordá que `x` es un vector de NumPy y por ende soporta los métodos `mean()` y `std()`. \n",
    "\n",
    "Volvé a responder las preguntas anteriores ahora con los datos normalizados.\n",
    "\n",
    "\n",
    "Nota: En este caso, tanto `w` como `x` ahora son vectores con 2 dimensiones, o sea `x` tiene tamaño `Nx2` y `w` tiene tamaño `2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Carga de datos\n",
    "data=rnutil.load_dataset_numpy(\"study_logistic_2d.csv\")\n",
    "x,y=data[:,0:2],data[:,2]\n",
    "\n",
    "## COMIENZO NORMALIZAR\n",
    "#completar normalizando los vectores x (ambas columnas)\n",
    "\n",
    "## FIN NORMALIZAR\n",
    "\n",
    "## Parámetros del modelo (probar valores entre -5 y 5)\n",
    "w=np.array([-0.2,-0.1])\n",
    "b=0\n",
    "\n",
    "# Configuración del descenso de gradiente\n",
    "\n",
    "#iteraciones máximas a realizar\n",
    "max_iterations=17000\n",
    "\n",
    "#velocidad de aprendizaje\n",
    "#valores sensatos: entre 0.00001 y 0.001\n",
    "alpha=0.001\n",
    "#fin Configuración\n",
    "\n",
    "\n",
    "# Inicialización del algoritmo\n",
    "iteration=0\n",
    "mean_error=0\n",
    "error_history=[]\n",
    "# Fin inicialización del algoritmo\n",
    "\n",
    "# Opciones de visualización \n",
    "fps=60\n",
    "#Fin opciones de visualización\n",
    "plot_state=init_plot(x,y,w,b)\n",
    "error_history=[]\n",
    "\n",
    "while iteration<max_iterations:\n",
    "    \n",
    "    #DESCENSO DE GRADIENTE\n",
    "    #predicciones del modelo\n",
    "\n",
    "    #calculo de derivadas\n",
    "    dEdw,dEdb=gradient(x,y,w,b)\n",
    "        \n",
    "    #CALCULO DE ERROR\n",
    "    #Calculo del error del modelo\n",
    "    yhat=predict(x,w,b)\n",
    "    error = binary_cross_entropy(y,yhat)\n",
    "    error_history.append(error)\n",
    "    #FIN CALCULO DE ERROR\n",
    "    \n",
    "    #visualización\n",
    "    if (iteration % 1000 ==0 or iteration == max_iterations-1):\n",
    "        visualizar(plot_state,x,y,iteration,w,b,dEdw,dEdb,max_iterations,error_history)\n",
    "        plt.pause(1/fps)\n",
    "    # fin visualización\n",
    "    \n",
    "    #actualizo los parámetros\n",
    "    w=w-alpha*dEdw\n",
    "    b=b-alpha*dEdb\n",
    "    \n",
    "    # FIN DESCENSO DE GRADIENTE\n",
    "    iteration+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
