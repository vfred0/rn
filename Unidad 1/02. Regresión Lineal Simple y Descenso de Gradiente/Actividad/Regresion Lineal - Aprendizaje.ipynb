{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"Regresion Lineal - Aprendizaje.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"ol8XNrB0EX9N"},"source":["# Visualización del proceso de aprendizaje en Regresión Lineal con descenso de gradiente\n","\n","Funciones auxiliares (solo ejecutar, y seguir más abajo)"]},{"cell_type":"code","metadata":{"id":"jNQMn5xyEX9X"},"source":["###############################################################################\n","%matplotlib notebook\n","from IPython.display import display, clear_output\n","import time\n","import matplotlib.pyplot as plt\n","from mpl_toolkits.mplot3d import Axes3D\n","import numpy as np\n","import os\n","import matplotlib.patches as patches\n","from collections import namedtuple\n","from matplotlib import cm\n","\n","\n","OptimizationState = namedtuple('OptimizationState', ['error_history','max_iterations'])\n","IterationState = namedtuple('IterationState', ['iteration','m','b','dEdm', 'dEdb','yhat','errors','mean_error'])\n","OptimalModel = namedtuple('Model', ['m', 'b','error'])\n","\n","PlotState = namedtuple('PlotState', ['figure','axes','legends'])\n","\n","## MODELO OPTIMO CON METODO ANALITICO DE SKLEARN\n","def optimal_model(x,y):\n","    from sklearn import datasets, linear_model\n","    regr = linear_model.LinearRegression()\n","    regr.fit(x.reshape(-1, 1),y)\n","    optimal_m=regr.coef_[0]\n","    optimal_b=regr.intercept_\n","    optimal_yhat=x*optimal_m+optimal_b\n","    optimal_error=((optimal_yhat-y)**2).mean()\n","    return OptimalModel(optimal_m,optimal_b,optimal_error)\n","    \n","## FUNCIONES DE DIBUJO\n","def init_data(ax_data,x,y,optimal,m,b):\n","    ax_data.scatter(x,y,color=\"blue\")\n","    \n","    x_pad=10\n","    min_x,max_x=x.min()-x_pad,x.max()+x_pad\n","    ax_data.plot([min_x,max_x],[min_x*optimal.m+optimal.b,max_x*optimal.m+optimal.b],color=\"cyan\")\n","    ax_data.plot([min_x,max_x],[min_x*m+b,max_x*m+b],color=\"red\")\n","    \n","    ax_data.set_xlabel(\"x (Horas estudiadas)\")\n","    ax_data.set_ylabel(\"y (Nota)\")\n","    \n","def init_error_history(ax_error):\n","    ax_error.set_xlabel(\"Iteración\")\n","    ax_error.set_ylabel(\"Error\")\n","    ax_error.set_title(\"Historial de Error\")\n","    \n","def init_contour(ax_contour,x,y,optimal,m,b):    \n","    ax_contour.set_xlabel(\"m\")\n","    ax_contour.set_ylabel(\"b\")\n","#     ax3.set_zlabel(\"E\")\n","    ax_contour.set_title(\"Contorno de E(m,b) \")\n","    detail=0.05\n","    param_range=8\n","    M = np.arange(-param_range, param_range, detail)\n","    B = np.arange(-param_range, param_range, detail)\n","    Ms, Bs = np.meshgrid(M, B)\n","    E=np.zeros_like(Ms)\n","    n=len(x)\n","    for i in range(n):\n","        Yi=Ms*x[i]+Bs\n","        E+=(Yi-y[i])**2\n","    E/=n\n","\n","    surf=ax_contour.contourf(Ms,Bs,E,cmap='coolwarm')\n","    \n","    ax_contour.scatter([optimal.m], [optimal.b],c=\"green\")\n","    plt.colorbar(surf, shrink=0.5, aspect=5)\n","    \n","def init_plot(x,y,optimal,m,b):\n","    \n","    figure=plt.figure(figsize=(12,5))\n","    plt.suptitle('Entrenamiento de modelo lineal')\n","    ax_data=figure.add_subplot(1,3,1)\n","    ax_error=figure.add_subplot(1,3,2)\n","    ax_contour=figure.add_subplot(1,3,3)\n","    axes=(ax_data,ax_error,ax_contour)\n","    \n","    init_data(ax_data,x,y,optimal,m,b)\n","    init_error_history(ax_error)\n","    init_contour(ax_contour,x,y,optimal,m,b)\n","    plt.tight_layout()\n","    \n","    \n","    \n","    return PlotState(figure,axes,None)\n","\n","def visualizar(plot_state,x,y,iteration_state,optimal,optimization_state):\n","    (ax1,ax2,ax3)=plot_state.axes\n","    # Visualizacion\n","    \n","    #actualizar linea del modelo actual\n","    ax1.lines.pop()\n","    x_pad=10\n","    min_x,max_x=x.min()-x_pad,x.max()+x_pad\n","    ax1.plot([min_x,max_x],[min_x*m+b,max_x*m+b],color=\"red\")\n","#     ax3.scatter(iteration_state.m, iteration_state.b, iteration_state.mean_error,c=\"black\",s=50)\n","    ax3.scatter([iteration_state.m], [iteration_state.b],c=\"black\",s=5)\n","    \n","    # Mostrar leyendas\n","    model = patches.Patch(color='red', label='Modelo: y=x*({:.2f})+({:.2f})'.format(iteration_state.m,iteration_state.b))\n","    model_true = patches.Patch(color='cyan', label='Modelo óptimo: y=x*({:.2f})+({:.2f})'.format(optimal.m,optimal.b))\n","    label='$\\\\frac{ \\\\partial E}{\\\\partial m}=$ %.2f, $\\\\frac{ \\\\partial E}{\\\\partial b}$ %.2f' %(iteration_state.dEdm,iteration_state.dEdb)\n","    derivatives = patches.Patch(color='red', label=label)\n","    error_patch = patches.Patch(color='red', label='$E=\\\\frac{1}{n} \\sum_i^n E_i=$ %.2f' % (iteration_state.mean_error))\n","    optimal_error = patches.Patch(color='cyan', label='E del modelo óptimo: %.2f' % (optimal.error))\n","    handles=[model,derivatives,error_patch,model_true,optimal_error]\n","    ax1.legend(handles=handles)\n","    \n","    ax2.lines.clear()\n","    ax2.plot(optimization_state.error_history,color=\"blue\")\n","    ax2.set_xlim(0,optimization_state.max_iterations)\n","    ax1.set_title(\"Iteración %03d / %03d\" % (iteration_state.iteration+1,optimization_state.max_iterations))\n","    plot_state.figure.canvas.draw()\n","        \n","    \n","\n","###############################################################################\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BhzbVXlnEX9a"},"source":["# Visualización del modelo y la superficie del error durante el aprendizaje\n","\n","El siguiente código entrena un modelo de regresión lineal `f(x)=mx+b` con descenso de gradiente.\n","\n","Para eso se muestran 3 gráficos durante la optimización:\n","* Los datos y el modelo, con los valores del error medio y las derivadas de los parámetros.\n","* La curva de error, que indica el error del modelo para cada iteración del algoritmo\n","* La superficie del error, que en este caso se muestra como el _contorno_ del error, es decir, como la superficie del error vista de arriba, y utilizando colores para indicar los valores del error.\n","\n","Estos gráficos se actualizan en cada iteración del algoritmo, de modo de poder ver el recorrido de los valores `m` y `b`, como eso afecta al modelo, y como va minimizándose el error en consecuencia.\n","\n","Probá modificando:\n","* Los valores iniciales de los parámetros `m` y `b`\n","* La tasa de aprendizaje `α`\n","* La cantidad de iteraciones máximas `max_iterations`\n","\n","Y comprendiendo como estos valores inciden en el algoritmo. Respondé:\n","\n","* ¿Qué valores de `α` permiten que el algoritmo converja? ¿y cuales que converja en un tiempo razonable?\n","* ¿Se alcanza siempre el mínimo global (punto verde)?\n","* ¿Qué parámetro importa más para la optimización? La magnitud de las derivadas ¿es la misma para ambos?\n","* ¿Se avanza lo mismo en todas las iteraciones (relacionar con magnitud de derivadas)?\n","\n","\n","Luego de eso, entre el comentario `## COMIENZO NORMALIZAR` y `##FIN NORMALIZAR`, normalizá los datos de entrada `x` restándoles la media μ y dividiendo por la desviación estándar σ con la fórmula `x ← (x-μ)/σ`. Recordá que `x` es un vector de NumPy y por ende soporta los métodos `mean()` y `std()`. \n","\n","Volvé a responder las preguntas anteriores ahora con los datos normalizados."]},{"cell_type":"code","metadata":{"id":"O22sYSDMEX9c"},"source":["\n","## Carga de datos\n","dataset_base=\"\"\n","dataset=\"study_regression_small.csv\"\n","#dataset=\"anscombe4.csv\"\n","\n","dataset_path=os.path.join(dataset_base,dataset)\n","data=np.loadtxt(open(dataset_path, \"rb\"), delimiter=\",\", skiprows=1)\n","x,y=data[:,0],data[:,1]\n","\n","## COMIENZO NORMALIZAR\n","#completar\n","\n","## FIN NORMALIZAR\n","\n","\n","## Parámetros iniciales del modelo (probar valores entre -5 y 5)\n","m=5\n","b=-4\n","# Configuración del descenso de gradiente\n","\n","#iteraciones máximas a realizar\n","max_iterations=400\n","#velocidad de aprendizaje\n","#valores sensatos: entre 0.00001 y 0.001 (pero probar otros también!)\n","alpha=0.0001\n","#fin Configuración\n","\n","# Cálculo del valor óptimo mediante cuadrados mínimos\n","optimal=optimal_model(x,y)\n","\n","# Inicialización del algoritmo\n","iteration=0\n","mean_error=0\n","error_history=[]\n","# Fin inicialización del algoritmo\n","\n","# Cuadros por segundo (velocidad de dibujado)\n","# Cuanto más grande, se pausa por menos tiempo entre cada iteración\n","# No modifica como funciona el algoritmo, solo la visualización\n","fps=5\n","\n","#Fin opciones de visualización\n","plot_state=init_plot(x,y,optimal,m,b)\n","optimization_state= OptimizationState(max_iterations=max_iterations,error_history=[])\n","\n","# Optimización con descenso de gradiente    \n","while iteration<optimization_state.max_iterations:\n","\n","    # Calcular predicciones y error\n","    #predicciones del modelo\n","    yhat=x*m+b\n","    # ERROR del modelo\n","    # error de cada ejemplo Ei\n","    errors= (y-yhat)**2\n","    # Error total E\n","    mean_error=errors.mean()\n","    \n","    #actualizar datos para plotear (no es necesario en el algoritmo)\n","    optimization_state.error_history.append(mean_error)\n","    \n","    #calculo de derivadas (para usar luego en descenso de gradiente)\n","    dEdm=2*((yhat-y)*x).mean()\n","    dEdb=2*(yhat-y).mean()\n","        \n","    #visualización cada 5 iteraciones (bajar 1 para que sea más frecuente)\n","    if (iteration % 5 == 0):\n","        # visualiamos \n","        iteration_state=IterationState(iteration,m,b,dEdm, dEdb,yhat,errors,mean_error)\n","        visualizar(plot_state,x,y,iteration_state,optimal,optimization_state)\n","        # pausar el algoritmo para que se pueda ver la actualización\n","        plt.pause(1/fps)\n","    # fin visualización\n","    \n","    #DESCENSO DE GRADIENTE\n","    #actualizo m\n","    m=m-alpha*dEdm\n","    #actualizo b\n","    b=b-alpha*dEdb\n","    # FIN DESCENSO DE GRADIENTE\n","    \n","    iteration+=1"],"execution_count":null,"outputs":[]}]}