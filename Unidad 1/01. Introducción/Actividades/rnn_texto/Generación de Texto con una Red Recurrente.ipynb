{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f197a8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import io\n",
    "\n",
    "# fijar la semilla\n",
    "# no es necesario en la práctica, \n",
    "# lo hacemos para poder replicar los experimentos\n",
    "tf.random.set_seed(0)\n",
    "np.random.seed(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419508f2",
   "metadata": {},
   "source": [
    "# Cargar los datos, y generar dataset de oraciones del texto\n",
    "\n",
    "Podés elegir entre 4 bases de datos de texto:\n",
    "* Nietzsche\n",
    "* Martín Fierro\n",
    "* Shakespeare (Sonetos)\n",
    "* Shakespeare (todo)\n",
    "\n",
    "O podés también buscar cualquier archivo de texto plano (`txt`) con texto y entrenar el modelo en base al mismo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "662aab3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de caracteres en el texto: 60620\n",
      "Longitud del alfabeto (caracteres únicos): 41\n",
      "Texto dividido en oraciones de longitud 40\n",
      "Cantidad de oraciones: 20194\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def read_text(path,n_sentence=40,step=3):\n",
    "    with io.open(path, encoding=\"utf-8\") as f:\n",
    "        text = f.read().lower()\n",
    "\n",
    "    text = text.replace(\"\\n\", \" \")  # quitar caracters de nueva linea\n",
    "    print(\"Cantidad de caracteres en el texto:\", len(text))\n",
    "\n",
    "    chars = sorted(list(set(text)))\n",
    "    print(\"Longitud del alfabeto (caracteres únicos):\", len(chars))\n",
    "    char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "    indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "    # cortar el texto en secuencias de longitud `n_sentence`\n",
    "    # que se superpongan `step` caracteres\n",
    "    sentences = []\n",
    "    next_chars = []\n",
    "    for i in range(0, len(text) - n_sentence, step):\n",
    "        sentences.append(text[i : i + n_sentence])\n",
    "        next_chars.append(text[i + n_sentence])\n",
    "    print(f\"Texto dividido en oraciones de longitud {n_sentence}\")\n",
    "    print(\"Cantidad de oraciones:\", len(sentences))\n",
    "\n",
    "    x = np.zeros((len(sentences), n_sentence, len(chars)), dtype=np.bool)\n",
    "    y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        for t, char in enumerate(sentence):\n",
    "            x[i, t, char_indices[char]] = 1\n",
    "        y[i, char_indices[next_chars[i]]] = 1\n",
    "    return x,y,char_indices,indices_char\n",
    "\n",
    "# Elegir una, el martin fierro, las obras de nietzsche, sonetos de shakespeare o todo shakespeare\n",
    "# Si se cambia el conjunto de datos debe volverse a correr el entrenamiento del modelo\n",
    "\n",
    "#path = \"nietzsche.txt\"\n",
    "path =\"martin_fierro.txt\"\n",
    "#path =\"shakespeare.txt\"\n",
    "#path =\"shakespeare_all.txt\"\n",
    "x, y, char_indices,indices_char = read_text(path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61896b1c",
   "metadata": {},
   "source": [
    "# Definir la Red Neuronal Recurrente para la generación (`model`) \n",
    "\n",
    "Y también funciones asociadas para generar nuevas oraciones (`generate_sentence`, `sample`, `fix_length`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1eee8025",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, randomness=1.0):\n",
    "    # Elige el próximo carácter a generar en base a \n",
    "    # las probabilidades de cada caracter generadas por la red\n",
    "    # El parámetro randomness indica la diversidad; a mayor valor\n",
    "    # la distribución se suaviza de forma de parecerse a una dist. uniforme\n",
    "    # y por ende el texto se hace más aleatorio\n",
    "    preds = np.asarray(preds).astype(\"float64\")\n",
    "    preds = np.log(preds) / randomness\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "def generate_sentence(model,length,initial_sentence,randomness,input_shape,char_indices,indices_char):\n",
    "    # utiliza la red para generar una secuencia nueva de texto\n",
    "    # de longitud `length` y en base a una oración inicial `initial_sentence`\n",
    "    n_sentence,n_alphabet = input_shape\n",
    "    generated = \"\"\n",
    "    initial_sentence = fix_length(initial_sentence,n_sentence)\n",
    "    for i in range(length):\n",
    "            x_pred = np.zeros((1, n_sentence,n_alphabet))\n",
    "            for t, char in enumerate(initial_sentence):\n",
    "                x_pred[0, t, char_indices[char]] = 1.0\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, randomness)\n",
    "            next_char = indices_char[next_index]\n",
    "            initial_sentence = initial_sentence[1:] + next_char\n",
    "            generated += next_char\n",
    "    return generated\n",
    "\n",
    "def fix_length(sentence,length):\n",
    "    n = len(sentence)\n",
    "    if n<length:\n",
    "        espacios =\" \"*(length-n)\n",
    "        return espacios + sentence\n",
    "    elif n > length:\n",
    "        return sentence[-length:]\n",
    "    else:\n",
    "        return sentence\n",
    "\n",
    "    \n",
    "# Definir el modelo de generación\n",
    "n,n_sentence,n_alphabet = x.shape\n",
    "# Definimos la red y la entrenamos con los datos\n",
    "input_shape = (n_sentence,n_alphabet)\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        layers.InputLayer(input_shape),\n",
    "        layers.LSTM(96),\n",
    "        layers.Dense(n_alphabet, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.01)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccf2782",
   "metadata": {},
   "source": [
    "# Entrenar el modelo de generación\n",
    "\n",
    "Podés interrumpir el proceso de entrenamiento para ver qué textos se están generando (celda de más abajo) y luego volver a correrlo (continua desde donde quedó, pero vuelve a contar la cantidad de épocas desde 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed86c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando por 99 épocas\n",
      "Epoch 1/99\n",
      "158/158 [==============================] - 8s 39ms/step - loss: 2.5249\n",
      "Epoch 2/99\n",
      "158/158 [==============================] - 6s 38ms/step - loss: 2.0783\n",
      "Epoch 3/99\n",
      "158/158 [==============================] - 6s 38ms/step - loss: 1.9430\n",
      "Epoch 4/99\n",
      "158/158 [==============================] - 6s 39ms/step - loss: 1.8628\n",
      "Epoch 5/99\n",
      "158/158 [==============================] - 6s 38ms/step - loss: 1.7972\n",
      "Epoch 6/99\n",
      "158/158 [==============================] - 6s 38ms/step - loss: 1.7298\n",
      "Epoch 7/99\n",
      "158/158 [==============================] - 6s 39ms/step - loss: 1.6777\n",
      "Epoch 8/99\n",
      "158/158 [==============================] - 6s 38ms/step - loss: 1.6214\n",
      "Epoch 9/99\n",
      "158/158 [==============================] - 6s 38ms/step - loss: 1.5667\n",
      "Epoch 10/99\n",
      "158/158 [==============================] - 6s 38ms/step - loss: 1.5152\n",
      "Epoch 11/99\n",
      "158/158 [==============================] - 6s 38ms/step - loss: 1.4707\n",
      "Epoch 12/99\n",
      "158/158 [==============================] - 6s 38ms/step - loss: 1.4252\n",
      "Epoch 13/99\n",
      "158/158 [==============================] - 6s 38ms/step - loss: 1.3765\n",
      "Epoch 14/99\n",
      "158/158 [==============================] - 6s 37ms/step - loss: 1.3369\n",
      "Epoch 15/99\n",
      "158/158 [==============================] - 6s 38ms/step - loss: 1.2958\n",
      "Epoch 16/99\n",
      "158/158 [==============================] - 6s 38ms/step - loss: 1.2560\n",
      "Epoch 17/99\n",
      "158/158 [==============================] - 6s 38ms/step - loss: 1.2288\n",
      "Epoch 18/99\n",
      "158/158 [==============================] - 6s 38ms/step - loss: 1.1899\n",
      "Epoch 19/99\n",
      "158/158 [==============================] - 6s 38ms/step - loss: 1.1715\n",
      "Epoch 20/99\n",
      "132/158 [========================>.....] - ETA: 0s - loss: 1.1252"
     ]
    }
   ],
   "source": [
    "# Calcular la cantidad de épocas o iteraciones de entrenamiento\n",
    "# en relación a la cantidad de oraciones (`n`) del conjunto de datos\n",
    "# En este caso elegimos que se entrene con 2.000.000 de iteraciones en total\n",
    "# Para entrenar correctamente deberíamos utilizar muchas más épocas (10 veces más)\n",
    "epochs = 2_000_000// n\n",
    "print(f\"Entrenando por {epochs} épocas\")\n",
    "# El entrenamiento dura aprox 10-20 minutos o más\n",
    "model.fit(x, y, batch_size=128, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb60ac33",
   "metadata": {},
   "source": [
    "# Generación de texto en base al modelo\n",
    "\n",
    "El modelo, sin recordar explícitamente el texto, puede generar ahora textos similares en estilo en base a las probabilidad de producir el caracter siguiente en una secuencia.  \n",
    "Para generar, hay que especificar:\n",
    "* `initial_sentence` Una oración inicial como contexto para que empiece a predecir (pueden ser todos espacios)\n",
    "* `sentence_length` Una cantidad de caracteres a generar\n",
    "* `randomness` Un valor que controla el suavizado de las probabilidades que genera la red de modo poder elegir qué tanto se desvía el texto generado de los patrones originales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d04d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elegir un valor de `randomness` entre 0 y 1 (o mayor a 1 pero va a ser muy aleatorio)\n",
    "# Valores más alto generan texto más aleatorio\n",
    "randomness = 0.3 \n",
    "# longitud del texto a generar en caracteres\n",
    "sentence_length = 400\n",
    "\n",
    "\n",
    "# oracion de comienzo para el generador\n",
    "# no utilizar acentos ni otros caracteres especiales (#!¿?,.)\n",
    "# tampoco usar mayúsculas\n",
    "initial_sentence = \"los hermanos sean unidos\" \n",
    "sentence = generate_sentence(model,sentence_length,initial_sentence,randomness,input_shape,char_indices,indices_char)\n",
    "print(f\"{initial_sentence} → {sentence}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
